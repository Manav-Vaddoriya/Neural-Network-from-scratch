#**Neural Network from Scratch (Python + NumPy)**#
ðŸ“Œ Overview

This project demonstrates how to build and train a simple feedforward neural network from scratch using only NumPy, without relying on deep learning libraries like TensorFlow or PyTorch.

The model is trained on the two-moons dataset from scikit-learn to perform binary classification. It uses forward propagation, backpropagation, gradient descent, and softmax cross-entropy loss.

ðŸš€ Features

* Implements a fully connected neural network using only NumPy.

* Uses tanh activation in the hidden layer.

* Softmax output layer for classification.

* Cross-entropy loss function with L2 regularization.

* Gradient descent optimization for weight updates.

* Visualizes the decision boundary after training.

âœ¨ Key Takeaway


This project shows how a neural network really works under the hood, giving you a solid understanding of forward propagation, backpropagation, and optimization â€” without relying on prebuilt libraries.
